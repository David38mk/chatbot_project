What’s a transformer?
A transfomer is a type of architecture used by LLMs for text processing and next word prediction based on a certain input. The output is a word generated on the following architecture: the transformer has vectors for each word in the input learned in the training process and updates those vectors based on two layers, the self attention layer and the feed forward neural network layer, after a couple iterations going through these types of layer sequentiually it gives a probability distribution on all the word in its dictionary and pick the most probable one to be next.


How does self-attention work?
The self attention block is one of the key building block in a transformer architecture, it takes each words embeding vector and updates it based on the context of the text surrounding it using multiple attention heads to addapt the current vector. Each attention head focuses on a different part of the text and analyses how this impacts the word in qustion. This is really importan in order for the model to undestrand links and meanings betfeen words. This works in the way that the model multiplies vectors and matrices. Using a value, key and query  weight matrice for each attention key multiplieng each by the embedding to get a query, key and value matrices which after that follow this math equation softmax(QxK)/sqrt(dim) * V to get z, which is the output of a single head of attention which is after that summed up wiht the other heads to update the vector of a single token and is fed to a FFNN 

What’s the difference between encoder-only, decoder-only, and encoder-decoder models?
Besides the architectural design (endocer-decoder has a stack of both in its architecture, ecnoder only has only and encoding-embeding capabilities, and decoder has only decoding abilities) these are all transformer models but specialised for different use the first for example can be used in text transaltion, the second qustion answering, sentimental analysis, and the third for chatbots and text generations. Also the diffrenece is in the way they use atention. The first ones use bidirectional attentions and get the full context of the input and thats how they update the words vector, but stil are a bit diffrent, and the third uses only unidirectional attention using only prior knoledge to extend on new one for example using the precoming words to generate a new one (it wouldnt be fare if it had the following words because that would not be word generation)


How does a transformer process a sentence?
A transformer procceses a sentence in the following way, it chunks it up into word (tokens) which all have initail vectors, then these vectors of the words pass sequentually  through an attention layer and then a FFNN, effectivly updaing the tokens vector in order to have a semanthical realtion with other tokens in the sentence and in order the model to "learn context of a sentence" because a mouse can have two meanings (computer mouse and the animal mouse), thats why its important for to take into consideration the other words in the sentence for the model to be able to predict the net word, give and answer to a qustion.. and so on. But back on topic after passing thorug the many layers of attention and FFNN the model is ready to give a prediction on the next word based of a probability distribution