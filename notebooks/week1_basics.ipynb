{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9566d900",
   "metadata": {},
   "source": [
    "# WEEK 1 - Learning the baics of LLMs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94833374",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "result = generator(\"Data structures and algorithms are\", max_length=40, num_return_sequences=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf36fd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data structures and algorithms are not compatible with the current software, and so the implementation will not be supported unless explicitly installed.\n",
      "\n",
      "This project, without any additional requirements, is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.\n",
      "\n",
      "This project is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.\n",
      "\n",
      "You should have received a copy of the GNU General Public License along with this project. If not, see <http://www.gnu.org/licenses/>.\n",
      "\n",
      "If you value this code, please do not use this code.\n",
      "\n",
      "<!-- See <http://www.gnu.org/licenses/> for details. -->\n",
      "\n",
      "The following sections contain the current version of the standard library.\n",
      "\n",
      "Documentation\n",
      "\n",
      "Copyright (C) 2006, 2006, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020,\n"
     ]
    }
   ],
   "source": [
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a3b6c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized IDs: tensor([[  33, 3219, 2989,  318,  257]])\n",
      "Decoded text: Binary search is a very useful tool for finding the right words to use in your search.\n",
      "\n",
      "The search engine is a very useful tool for finding the right words to use in your search. The search engine is a very useful tool for finding the right words to use in your search. The search engine is a very useful tool for finding the right words to use in your search. The search engine is a very useful tool for finding the right words to use in your search. The search engine\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "prompt = \"Binary search is a\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "print(\"Tokenized IDs:\", inputs[\"input_ids\"])\n",
    "\n",
    "outputs = model.generate(**inputs, max_length=100)\n",
    "print(\"Decoded text:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d090a3fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d0c34b28f544c886ad596bbe647155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\david\\.cache\\huggingface\\hub\\models--facebook--bart-large-cnn. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91742177ef64423d8a5e127f51e9c99e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd7e64af3e544ef2914bbb3f80b65466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60465fd417be4dcdbb5658f9612b5722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36368e84cc1342db8219d8b1f6a2718a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53d23c9d307e4b288acf466c4847ed9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Your min_length=56 must be inferior than your max_length=40.\n",
      "Your max_length is set to 40, but your input_length is only 8. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "C:\\Users\\david\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\generation\\utils.py:1633: UserWarning: Unfeasible length constraints: `min_length` (56) is larger than the maximum possible length (40). Generation will stop at the defined maximum length. You should decrease the minimum length and/or increase the maximum length.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'Transformers are neural networks that are connected to other neural networks. Transformers are the brainchild of computer scientists at the University of California, San Diego. They are the result of'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "summarizer(\"Transformers are neural networks that...\", max_length=40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78888d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Binary search is a very powerful tool, and it's very easy to use.\\n\\nThe first thing you need to do is to install the latest version of the Python interpreter.\\n\\nsudo apt-get install python-pip python-pip-dev python-pip-dev-tools python-pip-dev-tools-py2\\n\\nYou can also install the latest version of the Python interpreter from the command line.\\n\\nsudo apt-get install python-pip python-pip-dev python-pip-dev-tools python-pip-dev-tools-py2\\n\\nYou can also install the latest version of the Python interpreter from the command line.\\n\\nsudo apt-get install python-pip python-pip-dev python-pip-dev-tools python-pip-dev-tools-py2\\n\\nYou can also install the latest version of the Python interpreter from the command line.\\n\\nsudo apt-get install python-pip python-pip-dev python-pip-dev-tools python-pip-dev-tools-py2\\n\\nYou can also install the latest version of the Python interpreter from the command line.\\n\\nsudo apt-get install python\"}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(prompt, max_length=50, temperature=0.3, top_p=0.9)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
